{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e614dc2e",
   "metadata": {},
   "source": [
    "## Problem3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa44587",
   "metadata": {},
   "source": [
    "In this case study, you have been given Twitter data collected from an anonymous twitter handle. \n",
    "With the help of a Naïve Bayes model, predict if a given tweet about a real disaster is real or fake.\n",
    "1 = real tweet and 0 = fake tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b409d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "\n",
    "# Loading the data set\n",
    "tweets = pd.read_csv(\"Disaster_tweets_NB.csv\",encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b472491d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc4bed9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "873079fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2533"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['location'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2fd5a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3341"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['location'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2215ad5",
   "metadata": {},
   "source": [
    "location has 70% nulls and unique values. this would not give additional information. Hence we drop the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65b544d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can drop id as it does provide any additional info\n",
    "tweets.drop(['location'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "786f2c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['keyword'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d7e9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can delete these minimal number of rows\n",
    "tweets['keyword'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e51cceb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,\n",
       "              11,   12,   13,   14,   15,   16,   17,   18,   19,   20,   21,\n",
       "              22,   23,   24,   25,   26,   27,   28,   29,   30, 7583, 7584,\n",
       "            7585, 7586, 7587, 7588, 7589, 7590, 7591, 7592, 7593, 7594, 7595,\n",
       "            7596, 7597, 7598, 7599, 7600, 7601, 7602, 7603, 7604, 7605, 7606,\n",
       "            7607, 7608, 7609, 7610, 7611, 7612],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[tweets['keyword'].isnull()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4a46454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names of indexes for which column keyword has value null\n",
    "indexNames = tweets[tweets['keyword'].isnull()].index\n",
    "\n",
    "# Delete these row indexes from dataFrame\n",
    "tweets.drop(indexNames , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d28a2540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['keyword'].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83c176fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can drop id as it does provide any additional info\n",
    "tweets.drop(['id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9a8360c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Crying out for more! Set me ablaze</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword                                               text  target\n",
       "31  ablaze  @bbcmtd Wholesale Markets ablaze http://t.co/l...       1\n",
       "32  ablaze  We always try to bring the heavy. #metal #RT h...       0\n",
       "33  ablaze  #AFRICANBAZE: Breaking news:Nigeria flag set a...       1\n",
       "34  ablaze                 Crying out for more! Set me ablaze       0\n",
       "35  ablaze  On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "724b0ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning data \n",
    "import re\n",
    "stop_words = []\n",
    "# Load the custom built Stopwords\n",
    "with open(\"stopwords_en.txt\",\"r\") as sw:\n",
    "    stop_words = sw.read()\n",
    "\n",
    "stop_words = stop_words.split(\"\\n\")\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cb40a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " \"a's\",\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " \"ain't\",\n",
       " 'all',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'appear',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'are',\n",
       " \"aren't\",\n",
       " 'around',\n",
       " 'as',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'associated',\n",
       " 'at',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awfully',\n",
       " 'b',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'brief',\n",
       " 'but',\n",
       " 'by',\n",
       " 'c',\n",
       " \"c'mon\",\n",
       " \"c's\",\n",
       " 'came',\n",
       " 'can',\n",
       " \"can't\",\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'changes',\n",
       " 'clearly',\n",
       " 'co',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'concerning',\n",
       " 'consequently',\n",
       " 'consider',\n",
       " 'considering',\n",
       " 'contain',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'corresponding',\n",
       " 'could',\n",
       " \"couldn't\",\n",
       " 'course',\n",
       " 'currently',\n",
       " 'd',\n",
       " 'definitely',\n",
       " 'described',\n",
       " 'despite',\n",
       " 'did',\n",
       " \"didn't\",\n",
       " 'different',\n",
       " 'do',\n",
       " 'does',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " \"don't\",\n",
       " 'done',\n",
       " 'down',\n",
       " 'downwards',\n",
       " 'during',\n",
       " 'e',\n",
       " 'each',\n",
       " 'edu',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'enough',\n",
       " 'entirely',\n",
       " 'especially',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'ex',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'except',\n",
       " 'f',\n",
       " 'far',\n",
       " 'few',\n",
       " 'fifth',\n",
       " 'first',\n",
       " 'five',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forth',\n",
       " 'four',\n",
       " 'from',\n",
       " 'further',\n",
       " 'furthermore',\n",
       " 'g',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'greetings',\n",
       " 'h',\n",
       " 'had',\n",
       " \"hadn't\",\n",
       " 'happens',\n",
       " 'hardly',\n",
       " 'has',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he's\",\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " \"here's\",\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'hi',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hither',\n",
       " 'hopefully',\n",
       " 'how',\n",
       " 'howbeit',\n",
       " 'however',\n",
       " 'https',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ignored',\n",
       " 'immediate',\n",
       " 'in',\n",
       " 'inasmuch',\n",
       " 'inc',\n",
       " 'indeed',\n",
       " 'indicate',\n",
       " 'indicated',\n",
       " 'indicates',\n",
       " 'inner',\n",
       " 'insofar',\n",
       " 'instead',\n",
       " 'into',\n",
       " 'inward',\n",
       " 'is',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'j',\n",
       " 'just',\n",
       " 'k',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kept',\n",
       " 'know',\n",
       " 'knows',\n",
       " 'known',\n",
       " 'l',\n",
       " 'last',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'lest',\n",
       " 'let',\n",
       " \"let's\",\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likely',\n",
       " 'little',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'ltd',\n",
       " 'm',\n",
       " 'mainly',\n",
       " 'many',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'meanwhile',\n",
       " 'merely',\n",
       " 'might',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'n',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'nd',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needs',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'novel',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'o',\n",
       " 'obviously',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'overall',\n",
       " 'own',\n",
       " 'p',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'placed',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'possible',\n",
       " 'presumably',\n",
       " 'probably',\n",
       " 'provides',\n",
       " 'q',\n",
       " 'que',\n",
       " 'quite',\n",
       " 'qv',\n",
       " 'r',\n",
       " 'rather',\n",
       " 'rd',\n",
       " 're',\n",
       " 'really',\n",
       " 'reasonably',\n",
       " 'regarding',\n",
       " 'regardless',\n",
       " 'regards',\n",
       " 'relatively',\n",
       " 'respectively',\n",
       " 'right',\n",
       " 's',\n",
       " 'said',\n",
       " 'same',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'second',\n",
       " 'secondly',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'selves',\n",
       " 'sensible',\n",
       " 'sent',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'shall',\n",
       " 'she',\n",
       " 'should',\n",
       " \"shouldn't\",\n",
       " 'since',\n",
       " 'six',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'specified',\n",
       " 'specify',\n",
       " 'specifying',\n",
       " 'still',\n",
       " 'sub',\n",
       " 'such',\n",
       " 'sup',\n",
       " 'sure',\n",
       " 't',\n",
       " \"t's\",\n",
       " 'take',\n",
       " 'taken',\n",
       " 'tell',\n",
       " 'tends',\n",
       " 'th',\n",
       " 'than',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thanx',\n",
       " 'that',\n",
       " \"that's\",\n",
       " 'thats',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " \"there's\",\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'theres',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'think',\n",
       " 'third',\n",
       " 'this',\n",
       " 'thorough',\n",
       " 'thoroughly',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'took',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'tried',\n",
       " 'tries',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'twice',\n",
       " 'two',\n",
       " 'u',\n",
       " 'un',\n",
       " 'under',\n",
       " 'unfortunately',\n",
       " 'unless',\n",
       " 'unlikely',\n",
       " 'until',\n",
       " 'unto',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'uucp',\n",
       " 'v',\n",
       " 'value',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'viz',\n",
       " 'vs',\n",
       " 'w',\n",
       " 'want',\n",
       " 'wants',\n",
       " 'was',\n",
       " \"wasn't\",\n",
       " 'way',\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'welcome',\n",
       " 'well',\n",
       " 'went',\n",
       " 'were',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " \"what's\",\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " \"where's\",\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " \"who's\",\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'willing',\n",
       " 'wish',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " \"won't\",\n",
       " 'wonder',\n",
       " 'would',\n",
       " 'would',\n",
       " \"wouldn't\",\n",
       " 'x',\n",
       " 'y',\n",
       " 'yes',\n",
       " 'yet',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'z',\n",
       " 'zero']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a485f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(i):\n",
    "    i = re.sub(\"[^A-Za-z\" \"]+\",\" \",i).lower()\n",
    "  #  i = re.sub(\"[0-9\" \"]+\",\" \",i)\n",
    "    w = []\n",
    "    for word in i.split(\" \"):\n",
    "        if len(word)>3:\n",
    "            w.append(word)\n",
    "    return (\" \".join(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35c254e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hope having good week just checking'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing above function with sample text => removes punctuations, numbers\n",
    "cleaning_text(\"Hope you are having a good week. Just checking in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7dc825b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hope understand your feelings'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning_text(\"hope i can understand your feelings 123121. 123 hi how .. are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92334a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning_text(\"Hi how are you, I am good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d50abdff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31      @bbcmtd Wholesale Markets ablaze http://t.co/l...\n",
       "32      We always try to bring the heavy. #metal #RT h...\n",
       "33      #AFRICANBAZE: Breaking news:Nigeria flag set a...\n",
       "34                     Crying out for more! Set me ablaze\n",
       "35      On plus side LOOK AT THE SKY LAST NIGHT IT WAS...\n",
       "                              ...                        \n",
       "7578     @jt_ruff23 @cameronhacker and I wrecked you both\n",
       "7579    Three days off from work and they've pretty mu...\n",
       "7580    #FX #forex #trading Cramer: Iger's 3 words tha...\n",
       "7581    @engineshed Great atmosphere at the British Li...\n",
       "7582    Cramer: Iger's 3 words that wrecked Disney's s...\n",
       "Name: text, Length: 7552, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ee0747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.text = tweets.text.apply(cleaning_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4673aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31          bbcmtd wholesale markets ablaze http lhyxeohy\n",
       "32                     always bring heavy metal http xngw\n",
       "33      africanbaze breaking news nigeria flag ablaze ...\n",
       "34                                     crying more ablaze\n",
       "35         plus side look last night ablaze http qqsmshaj\n",
       "                              ...                        \n",
       "7578                      ruff cameronhacker wrecked both\n",
       "7579    three days from work they pretty much been wre...\n",
       "7580    forex trading cramer iger words that wrecked d...\n",
       "7581    engineshed great atmosphere british lion tonig...\n",
       "7582    cramer iger words that wrecked disney stock cn...\n",
       "Name: text, Length: 7552, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "390c389c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7552, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bc328e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7552, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no empty rows\n",
    "tweets.loc[tweets.text != \" \",:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59053164",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(tweets.iloc[:,:2]) # Predictors \n",
    "Y = np.array(tweets.iloc[:,2]) # Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2277f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "# Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "# splitting data into train and test data sets \n",
    "from sklearn.model_selection import train_test_split\n",
    "#tweets_train, tweets_test = train_test_split(tweets, test_size = 0.2)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "681d4bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['seismic',\n",
       "        'exploration takes seismic shift gabon somalia bloomberg http bekrpjnyhs somalia'],\n",
       "       ['screamed', 'mogacola zamtriossu screamed after hitting tweet'],\n",
       "       ['drown',\n",
       "        'this weekend nathan birthday weekend want drown yourself beer reckless things potentially'],\n",
       "       ...,\n",
       "       ['sirens', 'know dill pickle when taste'],\n",
       "       ['outbreak',\n",
       "        'person dies legionnaires disease outbreak http fjdm qhyai sebee'],\n",
       "       ['thunderstorm',\n",
       "        'continues severe thunderstorm warning oklahoma till http']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e8ca9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a matrix of token counts for the entire text document \n",
    "def split_into_words(i):\n",
    "    return [word for word in i.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f68b9d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the preparation of tweet texts into word count matrix format - Bag of Words\n",
    "tweets_bow = CountVectorizer(analyzer = split_into_words).fit(tweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54c8b2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=<function split_into_words at 0x000001927DF4D790>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc05fbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining BOW for all messages\n",
    "all_tweets_bow_matrix = tweets_bow.transform(tweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "15b5436a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['exploration takes seismic shift gabon somalia bloomberg http bekrpjnyhs somalia',\n",
       "       'mogacola zamtriossu screamed after hitting tweet',\n",
       "       'this weekend nathan birthday weekend want drown yourself beer reckless things potentially',\n",
       "       ..., 'know dill pickle when taste',\n",
       "       'person dies legionnaires disease outbreak http fjdm qhyai sebee',\n",
       "       'continues severe thunderstorm warning oklahoma till http'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3d776fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training messages\n",
    "train_tweets_matrix = tweets_bow.transform(X_train[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f4376f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing messages\n",
    "test_tweets_matrix = tweets_bow.transform(X_test[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1be8bead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1511, 19237)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweets_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9bf76b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Term weighting and normalizing on entire emails\n",
    "tfidf_transformer = TfidfTransformer().fit(all_tweets_bow_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b62876d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6041, 19237)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing TFIDF for train tweets\n",
    "train_tfidf = tfidf_transformer.transform(train_tweets_matrix)\n",
    "train_tfidf.shape # (row, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4e245442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1511, 19237)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing TFIDF for test tweets\n",
    "test_tfidf = tfidf_transformer.transform(test_tweets_matrix)\n",
    "test_tfidf.shape #  (row, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bf77bc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing a naive bayes model on training data set \n",
    "from sklearn.naive_bayes import MultinomialNB as MB\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "classifier_mb = MB()\n",
    "classifier_mb.fit(train_tfidf, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "70ed3944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.784910655195235"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation on Test Data\n",
    "test_pred_m = classifier_mb.predict(test_tfidf)\n",
    "accuracy_test_m = np.mean(test_pred_m == Y_test)\n",
    "accuracy_test_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "363794a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.784910655195235"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_pred_m, Y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c69072a",
   "metadata": {},
   "source": [
    "78% accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1e4813b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>779</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    0    1\n",
       "row_0          \n",
       "0      779  244\n",
       "1       81  407"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(test_pred_m, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mistakes made by the model on test data:\n",
    "    \n",
    "1. model predicted 81 fake tweets as real. (False Negative)\n",
    "2. model predicted 244 real tweets as fake. (False Positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f51162a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9036583347127959"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Data accuracy\n",
    "train_pred_m = classifier_mb.predict(train_tfidf)\n",
    "accuracy_train_m = np.mean(train_pred_m == Y_train)\n",
    "accuracy_train_m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badab8be",
   "metadata": {},
   "source": [
    "90% accuracy on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "eba0edf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=2)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Naive Bayes changing default alpha for laplace smoothing\n",
    "# if alpha = 0 then no smoothing is applied and the default alpha parameter is 1\n",
    "# the smoothing process mainly solves the emergence of zero probability problem in the dataset.\n",
    "classifier_mb_lap = MB(alpha = 2)\n",
    "classifier_mb_lap.fit(train_tfidf, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "43f69ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8750206919384208"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Data accuracy\n",
    "train_pred_lap = classifier_mb_lap.predict(train_tfidf)\n",
    "accuracy_train_lap = np.mean(train_pred_lap == Y_train)\n",
    "accuracy_train_lap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5e09ba9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7902051621442753"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation on Test Data after applying laplace\n",
    "test_pred_lap = classifier_mb_lap.predict(test_tfidf)\n",
    "accuracy_test_lap = np.mean(test_pred_lap == Y_test)\n",
    "accuracy_test_lap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03289e42",
   "metadata": {},
   "source": [
    "79% accuracy on test  data with laplace smoothening alpha = 2 . We are likely to predict tweet of a real disaster 79% times using this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0d82ee48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7902051621442753"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_pred_lap, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bb3be9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>803</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    0    1\n",
       "row_0          \n",
       "0      803  260\n",
       "1       57  391"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion matrix\n",
    "pd.crosstab(test_pred_lap, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca00748c",
   "metadata": {},
   "source": [
    "Mistakes made by the model on test data:\n",
    "    \n",
    "1. model predicted 57 fake tweets as real. (False Negative)\n",
    "2. model predicted 260 real tweets as fake. (False Positive)\n",
    "\n",
    "Though accuracy is better with laplace smoothening alpha = 2, we find False Positive less (244 vs 260) without smoothening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92941f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
